{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Define paths\n",
    "BASE_PATH = \"widsdatathon2025-university\"\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, \"train_tsv/train_tsv\")\n",
    "TEST_PATH = os.path.join(BASE_PATH, \"test_tsv/test_tsv\")\n",
    "METADATA_PATH = os.path.join(BASE_PATH, \"metadata\")\n",
    "DATA_PATH = \"data\"  # New folder for processed data\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_id(filename):\n",
    "    \"\"\"\n",
    "    Extract subject ID from the complex filename format\n",
    "    Example: sub-NDARAA075AMK_ses-HBNsiteSI_task-rest_acq-VARIANTObliquity_atlas-Schaefer2018p200n17_space-MNI152NLin6ASym_reg-36Parameter_desc-PearsonNilearn_correlations.tsv\n",
    "    \"\"\"\n",
    "    # Extract the subject ID (everything between 'sub-' and the first '_')\n",
    "    subject_id = filename.split('sub-')[1].split('_')[0]\n",
    "    return subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, encoders=None, is_training=True):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing including categorical variables\n",
    "    \"\"\"\n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    categorical_cols = ['sex', 'study_site', 'ethnicity', 'race', 'handedness', \n",
    "                       'parent_1_education', 'parent_2_education']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in data.columns:\n",
    "            if is_training:\n",
    "                # Create new encoder for training data\n",
    "                encoders[col] = LabelEncoder()\n",
    "                data[col] = encoders[col].fit_transform(data[col].fillna('missing'))\n",
    "            else:\n",
    "                # Use existing encoder for test data\n",
    "                data[col] = encoders[col].transform(data[col].fillna('missing'))\n",
    "    \n",
    "    # Handle numerical missing values\n",
    "    numerical_cols = ['bmi', 'p_factor_fs', 'internalizing_fs', 'externalizing_fs', 'attention_fs']\n",
    "    for col in numerical_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].fillna(data[col].mean())\n",
    "    \n",
    "    return data, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_connectome(file_path):\n",
    "    \"\"\"Load and process a single connectome matrix\"\"\"\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    upper_tri = matrix.values[np.triu_indices(200, k=1)]\n",
    "    return upper_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(folder_path, metadata_path=None):\n",
    "    \"\"\"Process dataset and save to file\"\"\"\n",
    "    features = []\n",
    "    subject_ids = []\n",
    "    \n",
    "    print(\"Processing connectome matrices...\")\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.tsv'):\n",
    "            subject_id = extract_subject_id(file)\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                connectome_features = load_and_process_connectome(file_path)\n",
    "                features.append(connectome_features)\n",
    "                subject_ids.append(subject_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_names = [f'feature_{i}' for i in range(len(features[0]))]\n",
    "    df = pd.DataFrame(features, columns=feature_names)\n",
    "    df['participant_id'] = subject_ids\n",
    "    \n",
    "    # Merge with metadata if provided\n",
    "    if metadata_path:\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        df = pd.merge(df, metadata, on='participant_id', how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_multiple_models(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"Train and evaluate multiple models\"\"\"\n",
    "    models = {\n",
    "        # 'RandomForest': {\n",
    "        #     'model': RandomForestRegressor(random_state=42),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [1000, 200],\n",
    "        #         'max_depth': [None, 10, 20],\n",
    "        #         'min_samples_split': [2, 5]\n",
    "        #     }\n",
    "        # },\n",
    "        # 'GradientBoosting': {\n",
    "        #     'model': GradientBoostingRegressor(random_state=42),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [1000, 200],\n",
    "        #         'learning_rate': [0.01, 0.1],\n",
    "        #         'max_depth': [3, 5]\n",
    "        #     }\n",
    "        # },\n",
    "        # 'XGBoost': {\n",
    "        #     'model': XGBRegressor(random_state=42),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [1000, 200],\n",
    "        #         'learning_rate': [0.01, 0.1],\n",
    "        #         'max_depth': [3, 5]\n",
    "        #     }\n",
    "        # },\n",
    "        'Lasso': {\n",
    "            'model': LassoCV(random_state=42),\n",
    "            'params': {}\n",
    "        },\n",
    "        'Ridge': {\n",
    "            'model': RidgeCV(),\n",
    "            'params': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model_info in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model = model_info['model']\n",
    "        params = model_info['params']\n",
    "        \n",
    "        if params:  # If there are parameters to tune\n",
    "            grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "        else:\n",
    "            best_model = model\n",
    "            best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_pred = best_model.predict(X_train)\n",
    "        val_pred = best_model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name] = {\n",
    "            'model': best_model,\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'val_r2': r2_score(y_val, val_pred),\n",
    "            'train_mse': mean_squared_error(y_train, train_pred),\n",
    "            'val_mse': mean_squared_error(y_val, val_pred),\n",
    "            'train_mae': mean_absolute_error(y_train, train_pred),\n",
    "            'val_mae': mean_absolute_error(y_val, val_pred)\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"Validation R²: {results[name]['val_r2']:.4f}\")\n",
    "        print(f\"Validation MSE: {results[name]['val_mse']:.4f}\")\n",
    "        print(f\"Validation MAE: {results[name]['val_mae']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(results):\n",
    "    \"\"\"Plot comparison of model performances\"\"\"\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Validation R²': [results[model]['val_r2'] for model in results],\n",
    "        'Validation MSE': [results[model]['val_mse'] for model in results]\n",
    "    })\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    sns.barplot(data=metrics_df, x='Model', y='Validation R²', ax=ax1)\n",
    "    ax1.set_title('Model Comparison - R²')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    sns.barplot(data=metrics_df, x='Model', y='Validation MSE', ax=ax2)\n",
    "    ax2.set_title('Model Comparison - MSE')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_PATH, 'model_comparison.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing connectome matrices...\n",
      "Processed data saved to data\n",
      "Processing connectome matrices...\n",
      "Processed data saved to data\n",
      "\n",
      "Training RandomForest...\n",
      "Best parameters for RandomForest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "RandomForest Results:\n",
      "Validation R²: 0.4806\n",
      "Validation MSE: 4.9538\n",
      "Validation MAE: 1.8486\n",
      "\n",
      "Training GradientBoosting...\n",
      "Best parameters for GradientBoosting: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000}\n",
      "GradientBoosting Results:\n",
      "Validation R²: 0.5507\n",
      "Validation MSE: 4.2853\n",
      "Validation MAE: 1.7142\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m X_val, _ \u001b[38;5;241m=\u001b[39m preprocess_data(X_val, encoders\u001b[38;5;241m=\u001b[39mencoders, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train and evaluate multiple models\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_multiple_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Plot model comparison\u001b[39;00m\n\u001b[0;32m     37\u001b[0m plot_model_comparison(results)\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mtrain_multiple_models\u001b[1;34m(X_train, X_val, y_train, y_val)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:  \u001b[38;5;66;03m# If there are parameters to tune\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rabia\\Documents\\.widscomp\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rabia\\Documents\\.widscomp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:933\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    929\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    931\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n\u001b[1;32m--> 933\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39m\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    934\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)\n\u001b[0;32m    936\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n",
      "File \u001b[1;32mc:\\Users\\rabia\\Documents\\.widscomp\\lib\\site-packages\\sklearn\\base.py:1237\u001b[0m, in \u001b[0;36mis_classifier\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rabia\\Documents\\.widscomp\\lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[0m, in \u001b[0;36mget_tags\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39mmro()):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_tags__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[1;32m--> 430\u001b[0m         sklearn_tags_provider[klass] \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    431\u001b[0m         class_order\u001b[38;5;241m.\u001b[39mappend(klass)\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_more_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "File \u001b[1;32mc:\\Users\\rabia\\Documents\\.widscomp\\lib\\site-packages\\sklearn\\base.py:613\u001b[0m, in \u001b[0;36mRegressorMixin.__sklearn_tags__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 613\u001b[0m     tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[0;32m    614\u001b[0m     tags\u001b[38;5;241m.\u001b[39mestimator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m     tags\u001b[38;5;241m.\u001b[39mregressor_tags \u001b[38;5;241m=\u001b[39m RegressorTags()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # #TRAIN DATA\n",
    "    # Process training data\n",
    "    print(\"Processing training data...\")\n",
    "    train_metadata_path = os.path.join(METADATA_PATH, \"training_metadata.csv\")\n",
    "    train_data = process_dataset(TRAIN_PATH, train_metadata_path)\n",
    "    \n",
    "    # Save processed data\n",
    "    train_data.to_csv(os.path.join(DATA_PATH, 'processed_train_data.csv'), index=False)\n",
    "    print(f\"Processed data saved to {DATA_PATH}\")\n",
    "    \n",
    "    # #TEST DATA\n",
    "    # Process testing data\n",
    "    test_metadata_path = os.path.join(METADATA_PATH, \"test_metadata.csv\")\n",
    "    test_data = process_dataset(TEST_PATH, test_metadata_path)\n",
    "    \n",
    "    # Save processed data\n",
    "    test_data.to_csv(os.path.join(DATA_PATH, 'processed_test_data.csv'), index=False)\n",
    "    print(f\"Processed data saved to {DATA_PATH}\")\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    feature_cols = [col for col in train_data.columns if col not in ['participant_id', 'age']]\n",
    "    X = train_data[feature_cols]\n",
    "    y = train_data['age']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, encoders = preprocess_data(X_train, is_training=True)\n",
    "    X_val, _ = preprocess_data(X_val, encoders=encoders, is_training=False)\n",
    "    \n",
    "    # Train and evaluate multiple models\n",
    "    results = train_multiple_models(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plot_model_comparison(results)\n",
    "    \n",
    "    # Save best model\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['val_r2'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    joblib.dump(best_model, os.path.join(DATA_PATH, 'best_model.joblib'))\n",
    "    joblib.dump(encoders, os.path.join(DATA_PATH, 'encoders.joblib'))\n",
    "    \n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    print(f\"Best model saved to {DATA_PATH}/best_model.joblib\")\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=importance_df.head(20), x='importance', y='feature')\n",
    "        plt.title('Top 20 Most Important Features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(DATA_PATH, 'feature_importance.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".widscomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
